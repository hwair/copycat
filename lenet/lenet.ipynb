{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0854c80b-4111-4d63-bf42-7209838e53bc",
   "metadata": {},
   "source": [
    "We implement LeNet-5 in its modern form after its evolution from LeCun's original design in the identification of handwritten zip codes. Our implementation is based on the architecture as presented by the following [image](https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg), modified slightly for a different input size. We apply it to the CIFAR10 dataset, mainly to see how it stacks up against more modern architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857d45cc-0381-44d1-8053-3f5105bf2780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Quadro T2000\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7831a62d-aa03-419b-8cab-fae26fe703c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    # in_channels: number of input channels\n",
    "    def __init__(self, in_channels: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Assume input is 32 x 32 x 3\n",
    "        # 32 x 32 x 6\n",
    "        self.conv0 = nn.Conv2d(in_channels, 6, kernel_size=5, padding=2)\n",
    "        # 16 x 16 x 6\n",
    "        self.avgpool0 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # 12 x 12 x 16\n",
    "        self.conv1 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        # 6 x 6 x 16\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # See linked image\n",
    "        self.layer0 = nn.Linear(576, 120)\n",
    "        self.layer1 = nn.Linear(120, 84)\n",
    "        self.layer2 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv0(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.avgpool0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = x.reshape(x.shape[0], (-1))\n",
    "        x = self.layer0(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90360bc4-b6c5-4b93-a7a3-9c6aa62d7328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training data: 45000\n",
      "Validation data: 5000\n",
      "Test data: 10000\n"
     ]
    }
   ],
   "source": [
    "transform_CIFAR10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Mean and standard deviation for CIFAR10 dataset\n",
    "    # Sourced from gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2470, 0.2435, 0.2616]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    transform=transform_CIFAR10,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    transform=transform_CIFAR10,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "VALID_SIZE = 0.9\n",
    "\n",
    "train_indices = list(range(len(train_data)))\n",
    "np.random.shuffle(train_indices)\n",
    "valid_split = int(len(train_data) * VALID_SIZE)\n",
    "valid_indices = train_indices[valid_split:]\n",
    "train_indices = train_indices[:valid_split]\n",
    "valid_data = Subset(train_data, valid_indices)\n",
    "train_data = Subset(train_data, train_indices)\n",
    "\n",
    "print(f\"Training data: {len(train_data)}\")\n",
    "print(f\"Validation data: {len(valid_data)}\")\n",
    "print(f\"Test data: {len(test_data)}\")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af1d30df-99d6-4434-bcbc-125c24ae0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = 10\n",
    "EPOCHS = 20\n",
    "LR = 0.1\n",
    "WEIGHT_DECAY = 0.001\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "model = LeNet(3)\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "metric = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    momentum=MOMENTUM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c99a43d9-2d60-48c3-a8fd-c9ce6ff6fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    metric: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> None:\n",
    "    total = len(loader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = metric(pred, y)\n",
    "\n",
    "        if batch % 100 == 99:\n",
    "            progress = (batch + 1) * len(x)\n",
    "            print(f\"\\tLoss: {loss.item():>7f} [{progress:>5d} / {total:>5d}]\")\n",
    "            print(f\"\\t\\tLearning rate: {optimizer.param_groups[0]['lr']:>8f}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "136537dd-4174-4a40-ad73-cf1af0ba7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    metric: nn.Module,\n",
    ") -> None:\n",
    "    total = len(loader.dataset)\n",
    "    batch_total = len(loader)\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(x)\n",
    "            total_loss += metric(pred, y).item()\n",
    "            pred_correct = pred.argmax(1) == y\n",
    "            total_correct += pred_correct.type(torch.float).sum().item()\n",
    "\n",
    "        total_loss /= batch_total\n",
    "        total_correct /= total\n",
    "        print(f\"\\tAccuracy: {(100 * total_correct):>0.1f}%\")\n",
    "        print(f\"\\tAverage loss: {total_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "554d75ff-2ae4-40b1-90e4-3464a3830413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tLoss: 2.326713 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.283743 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.315441 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.287435 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.318593 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.312937 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.315701 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 9.4%\n",
      "\tAverage loss: 2.303887\n",
      "Epoch: 2\n",
      "\tLoss: 2.307292 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.280637 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.091264 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.229997 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.960535 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.018706 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.129301 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 19.0%\n",
      "\tAverage loss: 2.074731\n",
      "Epoch: 3\n",
      "\tLoss: 1.974731 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.030190 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.033028 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.916725 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.788367 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.869736 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.837775 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 25.4%\n",
      "\tAverage loss: 1.942322\n",
      "Epoch: 4\n",
      "\tLoss: 1.794729 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.926662 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.810828 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.893040 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.145669 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.835268 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.793272 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 26.1%\n",
      "\tAverage loss: 1.887550\n",
      "Epoch: 5\n",
      "\tLoss: 1.799749 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.830871 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.840869 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.748467 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.835587 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.703776 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.599092 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 32.9%\n",
      "\tAverage loss: 1.747889\n",
      "Epoch: 6\n",
      "\tLoss: 1.885650 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.936207 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.602302 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.504973 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.618125 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.557250 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.642780 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 32.0%\n",
      "\tAverage loss: 1.746270\n",
      "Epoch: 7\n",
      "\tLoss: 1.769897 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.604951 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.627101 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.559049 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.497574 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.699978 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.450127 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 32.8%\n",
      "\tAverage loss: 1.704053\n",
      "Epoch: 8\n",
      "\tLoss: 1.620339 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.725031 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.787478 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.528838 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.503866 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.472744 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.715844 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 42.2%\n",
      "\tAverage loss: 1.571728\n",
      "Epoch: 9\n",
      "\tLoss: 1.490119 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.548487 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.514066 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.403981 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.368387 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.518873 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.608626 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 43.4%\n",
      "\tAverage loss: 1.566846\n",
      "Epoch: 10\n",
      "\tLoss: 1.497380 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.492447 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.540851 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.596303 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.564914 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.643710 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.561605 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 41.8%\n",
      "\tAverage loss: 1.541787\n",
      "Epoch: 11\n",
      "\tLoss: 1.645110 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.873503 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.540091 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.479493 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.410319 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.462044 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.390385 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 42.1%\n",
      "\tAverage loss: 1.541234\n",
      "Epoch: 12\n",
      "\tLoss: 1.669393 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.833856 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.425360 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.614225 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.528533 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.622065 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.484752 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 40.4%\n",
      "\tAverage loss: 1.591157\n",
      "Epoch: 13\n",
      "\tLoss: 1.415478 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.753484 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.489241 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.413224 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.449029 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.618509 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.543480 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 45.4%\n",
      "\tAverage loss: 1.514346\n",
      "Epoch: 14\n",
      "\tLoss: 1.349050 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.496392 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.346744 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.240181 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.381769 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.365997 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.561078 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 44.1%\n",
      "\tAverage loss: 1.506768\n",
      "Epoch: 15\n",
      "\tLoss: 1.462315 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.279915 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.792964 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.231105 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.392697 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.391505 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.339584 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 47.5%\n",
      "\tAverage loss: 1.440615\n",
      "Epoch: 16\n",
      "\tLoss: 1.329464 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.644204 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.279019 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.432265 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.134400 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.222904 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.423268 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 44.5%\n",
      "\tAverage loss: 1.496754\n",
      "Epoch: 17\n",
      "\tLoss: 1.432812 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.147762 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.455757 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.456529 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.420137 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.242060 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.272138 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 44.9%\n",
      "\tAverage loss: 1.489688\n",
      "Epoch: 18\n",
      "\tLoss: 1.302111 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.663768 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.283423 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.355803 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.381750 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.441162 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.164369 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 42.2%\n",
      "\tAverage loss: 1.727055\n",
      "Epoch: 19\n",
      "\tLoss: 1.206240 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.376661 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.467100 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.405556 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.380039 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.441531 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.292716 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 49.2%\n",
      "\tAverage loss: 1.398609\n",
      "Epoch: 20\n",
      "\tLoss: 1.314985 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.441129 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.323629 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.462930 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.397315 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.137092 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.310730 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 48.9%\n",
      "\tAverage loss: 1.397159\n"
     ]
    }
   ],
   "source": [
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch: {t + 1}\")\n",
    "    train(train_loader, model, metric, optimizer)\n",
    "    test(valid_loader, model, metric)\n",
    "\n",
    "torch.save(model.state_dict(), \"lenet5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1c34076-c0ff-43c1-9354-0814e6d1c121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 49.8%\n",
      "\tAverage loss: 1.362571\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, model, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4abf6b-bb25-4e28-8e61-30b52627403e",
   "metadata": {},
   "source": [
    "Considering the small architecture in comparison to the complexity of the dataset and the total lack of learning rate optimization, or that of any other hyperparameter, this is not bad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
