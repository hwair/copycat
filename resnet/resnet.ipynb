{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d37f1cc8",
   "metadata": {
    "id": "7ce0a3a0-ac42-4db3-98fc-b517969ea2f5"
   },
   "source": [
    "Using PyTorch, we implement the architectures for ResNet50, ResNet101, and ResNet152 as presented by He, Zhang, Ren, and Sun in their 2015 paper [*Deep Residual Learning for Image Recognition*](https://arxiv.org/pdf/1512.03385.pdf). We then proceed to train a ResNet50 model on the CIFAR10 dataset. We refer to both [Nouman](https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/)'s and [Persson](https://www.youtube.com/watch?v=DkNIBBBvcPs)'s implementations for guidance when necessary.\n",
    "\n",
    "In theory, an neural network can be expanded to an arbitrary depth by appending layers that learn just the identity function. In practice, the vanishing or exploding gradients arising from networks that are too deep impede the model's convergence. ResNet addresses this issue by adding in shortcut connections that add the input of a block directly to the output of a block. Then, instead of learning something close to identity function, a ResNet layer learns the deviation of the output from the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6decf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T17:45:13.266635Z",
     "iopub.status.busy": "2023-08-30T17:45:13.266351Z",
     "iopub.status.idle": "2023-08-30T17:45:20.142454Z",
     "shell.execute_reply": "2023-08-30T17:45:20.141421Z",
     "shell.execute_reply.started": "2023-08-30T17:45:13.266608Z"
    },
    "id": "61af13d7-41f6-4e7e-a29c-45dd05586a23"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99bd102",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-08-30T17:45:20.150181Z",
     "iopub.status.busy": "2023-08-30T17:45:20.147637Z",
     "iopub.status.idle": "2023-08-30T17:45:20.247271Z",
     "shell.execute_reply": "2023-08-30T17:45:20.246261Z",
     "shell.execute_reply.started": "2023-08-30T17:45:20.150144Z"
    },
    "id": "b0293001-81b7-4393-8c23-3ebf5cfb1674",
    "outputId": "7832bc82-50db-4846-d56a-813898fd2813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c0654e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T17:45:22.236220Z",
     "iopub.status.busy": "2023-08-30T17:45:22.235565Z",
     "iopub.status.idle": "2023-08-30T17:45:22.250400Z",
     "shell.execute_reply": "2023-08-30T17:45:22.249204Z",
     "shell.execute_reply.started": "2023-08-30T17:45:22.236184Z"
    },
    "id": "a53717c3-be2f-46e4-869f-59cd4ead51d2"
   },
   "outputs": [],
   "source": [
    "# Represents a single building block of a residual network\n",
    "class ResNetBlock(nn.Module):\n",
    "    # in_channels: number of input channels\n",
    "    # out_channels: number of output channels for intermediate layers\n",
    "    # downsample: sequence by which to reduce the identity map if necessary\n",
    "    # stride: stride of second conv of the three-layer stack forming a block\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        # This is by convention, see (Table 1) of the original paper\n",
    "        self.last_out_channels = out_channels * 4\n",
    "\n",
    "        self.conv0 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1\n",
    "        )\n",
    "        # Per the paper, we normalize after every conv and before activation\n",
    "        self.bn0 = nn.BatchNorm2d(out_channels)\n",
    "        # padding=1 adds back the two rows and columns lost by kernel_size=3\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            self.last_out_channels,\n",
    "            kernel_size=1\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(self.last_out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # In the forward of each block, the residual is added to the output\n",
    "        # Downsampling may be necessary to match the dimension of the residual\n",
    "        #     to that of the output\n",
    "        self.downsample_conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            self.last_out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=stride\n",
    "        )\n",
    "        self.downsample_bn = nn.BatchNorm2d(self.last_out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        x = self.conv0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        residual = self.downsample_conv(residual)\n",
    "        residual = self.downsample_bn(residual)\n",
    "\n",
    "        x += residual\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c29e715b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T17:45:24.405217Z",
     "iopub.status.busy": "2023-08-30T17:45:24.404492Z",
     "iopub.status.idle": "2023-08-30T17:45:24.431305Z",
     "shell.execute_reply": "2023-08-30T17:45:24.430251Z",
     "shell.execute_reply.started": "2023-08-30T17:45:24.405165Z"
    },
    "id": "6e5a3775-6997-4dd9-8039-fd76e8a6af0e"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    # in_channels: number of channels for an image in the dataset\n",
    "    # layers: list of length 4 representing the layers of ResNet blocks\n",
    "    # classes: number of output classes for the dataset\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        layers: list[int],\n",
    "        classes: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial two steps of convolution and maxpool for any ResNet\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            64,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Because maxpool already halves the output size, the layer represented\n",
    "        #     by layers[0] will only have stride=1\n",
    "        self.maxpool = nn.MaxPool2d(\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.layer0 = self._make_layer(layers[0], 64, 64, stride=1)\n",
    "        self.layer1 = self._make_layer(layers[1], 256, 128, stride=2)\n",
    "        self.layer2 = self._make_layer(layers[2], 512, 256, stride=2)\n",
    "        self.layer3 = self._make_layer(layers[3], 1024, 512, stride=2)\n",
    "\n",
    "        # Final two steps of avgpool and fully connected layer\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        # Number of output channels are scaled up by 4\n",
    "        self.fc = nn.Linear(2048, classes)\n",
    "\n",
    "    # Make a single layer of ResNet blocks\n",
    "    # num_blocks: number of blocks in the layer\n",
    "    # in_channels: number of input channels to the whole layer\n",
    "    # out_channels: number of output channels for intermediate convs of blocks\n",
    "    # stride: stride of first block of layer and of downsample if necessary\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        num_blocks: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1\n",
    "    ) -> nn.Module:\n",
    "        blocks = []\n",
    "        blocks.append(ResNetBlock(in_channels, out_channels, stride=stride))\n",
    "\n",
    "        # stride=1 for remaining blocks\n",
    "        for _ in range(num_blocks - 1):\n",
    "            blocks.append(ResNetBlock(out_channels * 4, out_channels))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        # Reshape for fully connected layer\n",
    "        x = x.reshape(x.shape[0], (-1))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ecdfa5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-08-30T17:46:32.519962Z",
     "iopub.status.busy": "2023-08-30T17:46:32.519583Z",
     "iopub.status.idle": "2023-08-30T17:46:40.608607Z",
     "shell.execute_reply": "2023-08-30T17:46:40.607625Z",
     "shell.execute_reply.started": "2023-08-30T17:46:32.519929Z"
    },
    "id": "98c69535-bba7-465d-af3a-442c58f16122",
    "outputId": "0adc12f1-cc6d-49fc-a8ed-a1d4f7510e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:04<00:00, 37764211.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n",
      "Training data: 45000\n",
      "Validation data: 5000\n",
      "Test data: 10000\n"
     ]
    }
   ],
   "source": [
    "transform_CIFAR10 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # Mean and standard deviation for CIFAR10 dataset\n",
    "    # Sourced from gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2470, 0.2435, 0.2616]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    transform=transform_CIFAR10,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    transform=transform_CIFAR10,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "VALID_SIZE = 0.9\n",
    "\n",
    "train_indices = list(range(len(train_data)))\n",
    "np.random.shuffle(train_indices)\n",
    "valid_split = int(len(train_data) * VALID_SIZE)\n",
    "valid_indices = train_indices[valid_split:]\n",
    "train_indices = train_indices[:valid_split]\n",
    "valid_data = Subset(train_data, valid_indices)\n",
    "train_data = Subset(train_data, train_indices)\n",
    "\n",
    "print(f\"Training data: {len(train_data)}\")\n",
    "print(f\"Validation data: {len(valid_data)}\")\n",
    "print(f\"Test data: {len(test_data)}\")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5c90a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T17:47:15.962075Z",
     "iopub.status.busy": "2023-08-30T17:47:15.961198Z",
     "iopub.status.idle": "2023-08-30T17:47:16.316702Z",
     "shell.execute_reply": "2023-08-30T17:47:16.315603Z",
     "shell.execute_reply.started": "2023-08-30T17:47:15.962033Z"
    },
    "id": "802aed2c-36f1-413e-b81d-9cb7a6ba2def"
   },
   "outputs": [],
   "source": [
    "RESNET50_LAYERS = [3, 4, 6, 3]\n",
    "RESNET101_LAYERS = [3, 4, 23, 3]\n",
    "RESNET152_LAYERS = [3, 8, 36, 3]\n",
    "\n",
    "CLASSES = 10\n",
    "EPOCHS = 20\n",
    "LR = 0.1\n",
    "WEIGHT_DECAY = 0.001\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# ResNet50 for CIFAR100\n",
    "model = ResNet(3, RESNET50_LAYERS, CLASSES)\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "metric = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    momentum=MOMENTUM\n",
    ")\n",
    "# Per the paper, \"the learning rate starts from 0.1 and is divided by 10\n",
    "#     when the error plateaus\"\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.1,\n",
    "    patience=8,\n",
    "    threshold=0.01,\n",
    "    threshold_mode=\"abs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "146c60f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T17:47:26.946181Z",
     "iopub.status.busy": "2023-08-30T17:47:26.945789Z",
     "iopub.status.idle": "2023-08-30T17:47:26.954741Z",
     "shell.execute_reply": "2023-08-30T17:47:26.953700Z",
     "shell.execute_reply.started": "2023-08-30T17:47:26.946148Z"
    },
    "id": "cee19dcc-34c5-4ca2-a6b3-35f05dfd0c5a"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    metric: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> None:\n",
    "    total = len(loader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = metric(pred, y)\n",
    "\n",
    "        if batch % 100 == 99:\n",
    "            scheduler.step(loss)\n",
    "            progress = (batch + 1) * len(x)\n",
    "            print(f\"\\tLoss: {loss.item():>7f} [{progress:>5d} / {total:>5d}]\")\n",
    "            print(f\"\\t\\tLearning rate: {optimizer.param_groups[0]['lr']:>8f}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e4f294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T17:47:33.615762Z",
     "iopub.status.busy": "2023-08-30T17:47:33.615410Z",
     "iopub.status.idle": "2023-08-30T17:47:33.624189Z",
     "shell.execute_reply": "2023-08-30T17:47:33.623059Z",
     "shell.execute_reply.started": "2023-08-30T17:47:33.615731Z"
    },
    "id": "0HlSxYK4i-eN"
   },
   "outputs": [],
   "source": [
    "def test(\n",
    "    loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    metric: nn.Module,\n",
    ") -> None:\n",
    "    total = len(loader.dataset)\n",
    "    batch_total = len(loader)\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(x)\n",
    "            total_loss += metric(pred, y).item()\n",
    "            pred_correct = pred.argmax(1) == y\n",
    "            total_correct += pred_correct.type(torch.float).sum().item()\n",
    "\n",
    "        total_loss /= batch_total\n",
    "        total_correct /= total\n",
    "        print(f\"\\tAccuracy: {(100 * total_correct):>0.1f}%\")\n",
    "        print(f\"\\tAverage loss: {total_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2c541c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-08-30T17:47:42.527491Z",
     "iopub.status.busy": "2023-08-30T17:47:42.527144Z",
     "iopub.status.idle": "2023-08-30T20:15:56.929577Z",
     "shell.execute_reply": "2023-08-30T20:15:56.928550Z",
     "shell.execute_reply.started": "2023-08-30T17:47:42.527463Z"
    },
    "id": "3142b016-f03e-4944-a1e4-a0ea5a425f06",
    "outputId": "13be619a-6e7e-476f-b2c8-236f5968da04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tLoss: 2.078911 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 2.078148 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.936066 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.939066 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.808320 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.854816 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.799024 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 30.2%\n",
      "\tAverage loss: 1.787851\n",
      "Epoch: 2\n",
      "\tLoss: 1.773424 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.641893 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.618807 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.633263 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.836801 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.670297 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.529169 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 38.9%\n",
      "\tAverage loss: 1.636970\n",
      "Epoch: 3\n",
      "\tLoss: 1.665519 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.688651 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.560869 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.384383 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.637148 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.609859 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.349661 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 41.6%\n",
      "\tAverage loss: 1.596656\n",
      "Epoch: 4\n",
      "\tLoss: 1.503148 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.831711 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.576255 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.299843 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.453941 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.254647 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.237570 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 48.0%\n",
      "\tAverage loss: 1.386860\n",
      "Epoch: 5\n",
      "\tLoss: 1.387048 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.256251 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.419975 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.140740 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.252100 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.055199 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.302328 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 41.7%\n",
      "\tAverage loss: 1.941293\n",
      "Epoch: 6\n",
      "\tLoss: 1.404406 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.219382 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.213516 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.146638 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.225532 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.463621 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.090462 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 40.9%\n",
      "\tAverage loss: 1.680162\n",
      "Epoch: 7\n",
      "\tLoss: 0.899406 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.286700 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.076894 [19200 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 0.992990 [25600 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.235510 [32000 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.185940 [38400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.270619 [44800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tAccuracy: 46.3%\n",
      "\tAverage loss: 1.581015\n",
      "Epoch: 8\n",
      "\tLoss: 1.073009 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.282289 [12800 / 45000]\n",
      "\t\tLearning rate: 0.100000\n",
      "\tLoss: 1.142855 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.957895 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.745050 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.862431 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.608971 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 74.4%\n",
      "\tAverage loss: 0.734134\n",
      "Epoch: 9\n",
      "\tLoss: 0.824244 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.635544 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.730075 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.523235 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.618128 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.593228 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.445707 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 77.1%\n",
      "\tAverage loss: 0.672920\n",
      "Epoch: 10\n",
      "\tLoss: 0.661430 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.427221 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.517274 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.824263 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.710010 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.553007 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.608370 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 77.0%\n",
      "\tAverage loss: 0.667613\n",
      "Epoch: 11\n",
      "\tLoss: 0.434405 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.526289 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.576761 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.582548 [25600 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.396727 [32000 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.424116 [38400 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.502263 [44800 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tAccuracy: 81.2%\n",
      "\tAverage loss: 0.556956\n",
      "Epoch: 12\n",
      "\tLoss: 0.510372 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.370780 [12800 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.281941 [19200 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.334823 [25600 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.277601 [32000 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.372064 [38400 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.474611 [44800 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tAccuracy: 82.0%\n",
      "\tAverage loss: 0.544627\n",
      "Epoch: 13\n",
      "\tLoss: 0.413769 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.444555 [12800 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.386591 [19200 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.582163 [25600 / 45000]\n",
      "\t\tLearning rate: 0.001000\n",
      "\tLoss: 0.479599 [32000 / 45000]\n",
      "\t\tLearning rate: 0.000100\n",
      "\tLoss: 0.464568 [38400 / 45000]\n",
      "\t\tLearning rate: 0.000100\n",
      "\tLoss: 0.351005 [44800 / 45000]\n",
      "\t\tLearning rate: 0.000100\n",
      "\tAccuracy: 82.0%\n",
      "\tAverage loss: 0.538394\n",
      "Epoch: 14\n",
      "\tLoss: 0.506490 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.000100\n",
      "\tLoss: 0.287663 [12800 / 45000]\n",
      "\t\tLearning rate: 0.000100\n",
      "\tLoss: 0.429652 [19200 / 45000]\n",
      "\t\tLearning rate: 0.000100\n",
      "\tLoss: 0.556843 [25600 / 45000]\n",
      "\t\tLearning rate: 0.000100\n",
      "\tLoss: 0.277644 [32000 / 45000]\n",
      "\t\tLearning rate: 0.000100\n",
      "\tLoss: 0.334257 [38400 / 45000]\n",
      "\t\tLearning rate: 0.000100\n",
      "\tLoss: 0.405956 [44800 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tAccuracy: 82.2%\n",
      "\tAverage loss: 0.534884\n",
      "Epoch: 15\n",
      "\tLoss: 0.299069 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tLoss: 0.299620 [12800 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tLoss: 0.256409 [19200 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tLoss: 0.371230 [25600 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tLoss: 0.441081 [32000 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tLoss: 0.578753 [38400 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tLoss: 0.326856 [44800 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tAccuracy: 82.1%\n",
      "\tAverage loss: 0.537057\n",
      "Epoch: 16\n",
      "\tLoss: 0.338455 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tLoss: 0.363916 [12800 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tLoss: 0.289922 [19200 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tLoss: 0.436890 [25600 / 45000]\n",
      "\t\tLearning rate: 0.000010\n",
      "\tLoss: 0.259881 [32000 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.379219 [38400 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.455055 [44800 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tAccuracy: 82.2%\n",
      "\tAverage loss: 0.533796\n",
      "Epoch: 17\n",
      "\tLoss: 0.487513 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.274369 [12800 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.310606 [19200 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.385164 [25600 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.553845 [32000 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.529625 [38400 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.234065 [44800 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tAccuracy: 82.5%\n",
      "\tAverage loss: 0.534131\n",
      "Epoch: 18\n",
      "\tLoss: 0.631061 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.419005 [12800 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.433979 [19200 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.298187 [25600 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.414287 [32000 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.337958 [38400 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.341545 [44800 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tAccuracy: 82.2%\n",
      "\tAverage loss: 0.538405\n",
      "Epoch: 19\n",
      "\tLoss: 0.451124 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.000001\n",
      "\tLoss: 0.498027 [12800 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tLoss: 0.317052 [19200 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tLoss: 0.588294 [25600 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tLoss: 0.343316 [32000 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tLoss: 0.422272 [38400 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tLoss: 0.413761 [44800 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tAccuracy: 82.4%\n",
      "\tAverage loss: 0.533431\n",
      "Epoch: 20\n",
      "\tLoss: 0.481176 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tLoss: 0.285637 [12800 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tLoss: 0.460468 [19200 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tLoss: 0.406106 [25600 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tLoss: 0.376016 [32000 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tLoss: 0.307094 [38400 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tLoss: 0.373109 [44800 / 45000]\n",
      "\t\tLearning rate: 0.000000\n",
      "\tAccuracy: 82.6%\n",
      "\tAverage loss: 0.535124\n"
     ]
    }
   ],
   "source": [
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch: {t + 1}\")\n",
    "    train(train_loader, model, metric, optimizer)\n",
    "    test(valid_loader, model, metric)\n",
    "\n",
    "torch.save(model.state_dict(), \"resnet50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79f79a2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T20:19:11.766629Z",
     "iopub.status.busy": "2023-08-30T20:19:11.765792Z",
     "iopub.status.idle": "2023-08-30T20:19:58.612868Z",
     "shell.execute_reply": "2023-08-30T20:19:58.611915Z",
     "shell.execute_reply.started": "2023-08-30T20:19:11.766593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 81.5%\n",
      "\tAverage loss: 0.538406\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, model, metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce1e9fb9",
   "metadata": {},
   "source": [
    "We note that our accuracy is relatively low for the given dataset due to our implementation of an unmodified ResNet50. In particular, there are far too many parameters compared to the size of each image in CIFAR10."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
