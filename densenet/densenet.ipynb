{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e9859e-6447-440a-87c4-d39a8493320e",
   "metadata": {},
   "source": [
    "We implement a slight modification of DenseNet-121, following the 2018 [paper](https://arxiv.org/pdf/1608.06993.pdf) by Huang, Liu, van der Maaten, and Weinberger.\n",
    "\n",
    "Addressing the same problems arising from increasing network depth as ResNet, DenseNet opts instead for concatenation in its equivalent of a shortcut connection, in contrast to ResNet's concatenation. Effectively, as the layers in a dense block append their output onto their input, it is as though a layer feeds its output directly into each subsequent layer, hence the name \"dense\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c3b0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-13T23:29:32.673645Z",
     "iopub.status.busy": "2023-09-13T23:29:32.672876Z",
     "iopub.status.idle": "2023-09-13T23:29:37.207538Z",
     "shell.execute_reply": "2023-09-13T23:29:37.206298Z",
     "shell.execute_reply.started": "2023-09-13T23:29:32.673610Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578c9585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-13T23:29:37.210011Z",
     "iopub.status.busy": "2023-09-13T23:29:37.209549Z",
     "iopub.status.idle": "2023-09-13T23:29:37.252293Z",
     "shell.execute_reply": "2023-09-13T23:29:37.251137Z",
     "shell.execute_reply.started": "2023-09-13T23:29:37.209973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3260af0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T01:31:48.161817Z",
     "iopub.status.busy": "2023-09-14T01:31:48.161441Z",
     "iopub.status.idle": "2023-09-14T01:31:48.172711Z",
     "shell.execute_reply": "2023-09-14T01:31:48.171703Z",
     "shell.execute_reply.started": "2023-09-14T01:31:48.161788Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    # in_channels: number of input channels\n",
    "    # num_layers: number of layers in the dense block\n",
    "    # growth_rate: denoted k in the paper\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        num_layers: int,\n",
    "        growth_rate: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        for n in range(num_layers):\n",
    "            # 1x1 conv followed by 3x3 conv, each with batchnorm and relu\n",
    "            self.layers.append(nn.Sequential(\n",
    "                # Adding nk is the result of the concatenation\n",
    "                nn.BatchNorm2d(in_channels + n * growth_rate),\n",
    "                nn.ReLU(),\n",
    "                # Each 1x1 produces 4k output channels per the paper\n",
    "                nn.Conv2d(\n",
    "                    in_channels + n * growth_rate,\n",
    "                    4 * growth_rate,\n",
    "                    kernel_size=1\n",
    "                ),\n",
    "                nn.BatchNorm2d(4 * growth_rate),\n",
    "                nn.ReLU(),\n",
    "                # Output k channels\n",
    "                nn.Conv2d(\n",
    "                    4 * growth_rate, growth_rate,\n",
    "                    kernel_size=3, padding=1\n",
    "                )\n",
    "            ))\n",
    "            \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for l in self.layers:\n",
    "            x = torch.cat((x, l(x)), 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "991af410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T01:31:48.733711Z",
     "iopub.status.busy": "2023-09-14T01:31:48.733021Z",
     "iopub.status.idle": "2023-09-14T01:31:48.741433Z",
     "shell.execute_reply": "2023-09-14T01:31:48.740506Z",
     "shell.execute_reply.started": "2023-09-14T01:31:48.733678Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransitionLayer(nn.Module):\n",
    "    # in_channels: number of input channels\n",
    "    # compression: denoted theta in the paper\n",
    "    def __init__(self, in_channels: int, compression: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            int(in_channels * compression),\n",
    "            kernel_size=1\n",
    "        )\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9544452f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T01:32:08.829271Z",
     "iopub.status.busy": "2023-09-14T01:32:08.828914Z",
     "iopub.status.idle": "2023-09-14T01:32:08.840906Z",
     "shell.execute_reply": "2023-09-14T01:32:08.839866Z",
     "shell.execute_reply.started": "2023-09-14T01:32:08.829241Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    # in_channels: number of input channels\n",
    "    # layers: list of number of layers per dense block\n",
    "    # growth_rate: denoted k in the paper\n",
    "    # compression: denoted theta in the paper\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        layers: list[int],\n",
    "        growth_rate: int,\n",
    "        compression: float\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        num_ch = 64\n",
    "        self.components = []\n",
    "\n",
    "        # Preprocessing, with some parameters modified to acount for data size\n",
    "        # In particular, we apply no stride\n",
    "        self.conv = nn.Conv2d(in_channels, num_ch, kernel_size=5, padding=2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Add in blocks and transition layers\n",
    "        for i, num_layers in enumerate(layers):\n",
    "            self.components.append(DenseBlock(num_ch, num_layers, growth_rate))\n",
    "            num_ch += num_layers * growth_rate\n",
    "\n",
    "            # Transition for all but last block\n",
    "            if i != len(layers) - 1:\n",
    "                self.components.append(TransitionLayer(num_ch, compression))\n",
    "                num_ch = int(num_ch * compression)\n",
    "\n",
    "        self.components = nn.ModuleList(self.components)\n",
    "                \n",
    "        # Postprocessing\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=4, stride=1)\n",
    "        self.fc = nn.Linear(num_ch, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        for component in self.components:\n",
    "            x = component(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], (-1))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd585865",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-13T23:29:41.671330Z",
     "iopub.status.busy": "2023-09-13T23:29:41.670964Z",
     "iopub.status.idle": "2023-09-13T23:29:51.986187Z",
     "shell.execute_reply": "2023-09-13T23:29:51.983204Z",
     "shell.execute_reply.started": "2023-09-13T23:29:41.671284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:05<00:00, 31855805.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n",
      "Training data: 45000\n",
      "Validation data: 5000\n",
      "Test data: 10000\n"
     ]
    }
   ],
   "source": [
    "transform_CIFAR10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Mean and standard deviation for CIFAR10 dataset\n",
    "    # Sourced from gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2470, 0.2435, 0.2616]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    transform=transform_CIFAR10,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    transform=transform_CIFAR10,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "VALID_SIZE = 0.9\n",
    "\n",
    "train_indices = list(range(len(train_data)))\n",
    "np.random.shuffle(train_indices)\n",
    "valid_split = int(len(train_data) * VALID_SIZE)\n",
    "valid_indices = train_indices[valid_split:]\n",
    "train_indices = train_indices[:valid_split]\n",
    "valid_data = Subset(train_data, valid_indices)\n",
    "train_data = Subset(train_data, train_indices)\n",
    "\n",
    "print(f\"Training data: {len(train_data)}\")\n",
    "print(f\"Validation data: {len(valid_data)}\")\n",
    "print(f\"Test data: {len(test_data)}\")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0f094a0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T02:26:56.381526Z",
     "iopub.status.busy": "2023-09-14T02:26:56.381046Z",
     "iopub.status.idle": "2023-09-14T02:26:56.492980Z",
     "shell.execute_reply": "2023-09-14T02:26:56.491966Z",
     "shell.execute_reply.started": "2023-09-14T02:26:56.381488Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "LR = 0.01\n",
    "WEIGHT_DECAY = 0.001\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "model = DenseNet(3, [6, 12, 24, 16], 12, 0.5)\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "metric = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    momentum=MOMENTUM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "92fa6216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T02:26:56.765947Z",
     "iopub.status.busy": "2023-09-14T02:26:56.765565Z",
     "iopub.status.idle": "2023-09-14T02:26:56.774633Z",
     "shell.execute_reply": "2023-09-14T02:26:56.773581Z",
     "shell.execute_reply.started": "2023-09-14T02:26:56.765916Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    metric: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> None:\n",
    "    total = len(loader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = metric(pred, y)\n",
    "\n",
    "        if batch % 100 == 99:\n",
    "            progress = (batch + 1) * len(x)\n",
    "            print(f\"\\tLoss: {loss.item():>7f} [{progress:>5d} / {total:>5d}]\")\n",
    "            print(f\"\\t\\tLearning rate: {optimizer.param_groups[0]['lr']:>8f}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b44195cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T02:26:57.157659Z",
     "iopub.status.busy": "2023-09-14T02:26:57.157222Z",
     "iopub.status.idle": "2023-09-14T02:26:57.167581Z",
     "shell.execute_reply": "2023-09-14T02:26:57.166205Z",
     "shell.execute_reply.started": "2023-09-14T02:26:57.157625Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(\n",
    "    loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    metric: nn.Module,\n",
    ") -> None:\n",
    "    total = len(loader.dataset)\n",
    "    batch_total = len(loader)\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(x)\n",
    "            total_loss += metric(pred, y).item()\n",
    "            pred_correct = pred.argmax(1) == y\n",
    "            total_correct += pred_correct.type(torch.float).sum().item()\n",
    "\n",
    "        total_loss /= batch_total\n",
    "        total_correct /= total\n",
    "        print(f\"\\tAccuracy: {(100 * total_correct):>0.1f}%\")\n",
    "        print(f\"\\tAverage loss: {total_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "984639ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T02:26:57.809382Z",
     "iopub.status.busy": "2023-09-14T02:26:57.808654Z",
     "iopub.status.idle": "2023-09-14T02:46:00.208415Z",
     "shell.execute_reply": "2023-09-14T02:46:00.207357Z",
     "shell.execute_reply.started": "2023-09-14T02:26:57.809338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tLoss: 1.754052 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 1.485655 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 1.486960 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 1.320729 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 1.134369 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 1.221157 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 1.081045 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 54.4%\n",
      "\tAverage loss: 1.290990\n",
      "Epoch: 2\n",
      "\tLoss: 1.136029 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 1.096946 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.888631 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.912430 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 1.046178 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.918515 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.998655 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 66.7%\n",
      "\tAverage loss: 0.946098\n",
      "Epoch: 3\n",
      "\tLoss: 0.714837 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.622070 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.851592 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.725235 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.620355 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.669699 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.837988 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 70.0%\n",
      "\tAverage loss: 0.842305\n",
      "Epoch: 4\n",
      "\tLoss: 0.964403 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.581767 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.877238 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.747746 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.638389 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.554674 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.704536 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 74.1%\n",
      "\tAverage loss: 0.733376\n",
      "Epoch: 5\n",
      "\tLoss: 0.395474 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.492728 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.713099 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.453602 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.512573 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.669413 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.659158 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 78.5%\n",
      "\tAverage loss: 0.631363\n",
      "Epoch: 6\n",
      "\tLoss: 0.406673 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.447147 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.506166 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.408186 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.411229 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.579375 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.571675 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 74.5%\n",
      "\tAverage loss: 0.749626\n",
      "Epoch: 7\n",
      "\tLoss: 0.579321 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.430647 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.457693 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.394002 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.586430 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.269892 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.706826 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 77.6%\n",
      "\tAverage loss: 0.669507\n",
      "Epoch: 8\n",
      "\tLoss: 0.364093 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.363026 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.416107 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.331849 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.406390 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.349602 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.313568 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 80.2%\n",
      "\tAverage loss: 0.585889\n",
      "Epoch: 9\n",
      "\tLoss: 0.519866 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.437993 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.316865 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.395963 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.273808 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.305425 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.366407 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 81.7%\n",
      "\tAverage loss: 0.545597\n",
      "Epoch: 10\n",
      "\tLoss: 0.239380 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.288249 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.326538 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.382784 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.289105 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.390702 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.401966 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 78.8%\n",
      "\tAverage loss: 0.705071\n",
      "Epoch: 11\n",
      "\tLoss: 0.374002 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.307638 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.279941 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.218809 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.229827 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.576480 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.386486 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 81.5%\n",
      "\tAverage loss: 0.577276\n",
      "Epoch: 12\n",
      "\tLoss: 0.213766 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.251193 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.293926 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.367478 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.403329 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.274633 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.394942 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 82.6%\n",
      "\tAverage loss: 0.524001\n",
      "Epoch: 13\n",
      "\tLoss: 0.290448 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.263582 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.208559 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.308968 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.505679 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.210629 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.323557 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 81.7%\n",
      "\tAverage loss: 0.573623\n",
      "Epoch: 14\n",
      "\tLoss: 0.245821 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.362154 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.345882 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.207889 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.080578 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.360437 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.146136 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 81.1%\n",
      "\tAverage loss: 0.598420\n",
      "Epoch: 15\n",
      "\tLoss: 0.146474 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.103120 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.086009 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.319906 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.261338 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.225182 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.492741 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 82.5%\n",
      "\tAverage loss: 0.555241\n",
      "Epoch: 16\n",
      "\tLoss: 0.211059 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.152104 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.293692 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.378635 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.345838 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.263481 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.300661 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 79.7%\n",
      "\tAverage loss: 0.642537\n",
      "Epoch: 17\n",
      "\tLoss: 0.265642 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.145603 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.396586 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.448543 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.101887 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.235565 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.420162 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 81.7%\n",
      "\tAverage loss: 0.598478\n",
      "Epoch: 18\n",
      "\tLoss: 0.149575 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.125595 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.163869 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.269796 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.215986 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.240799 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.254941 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 82.4%\n",
      "\tAverage loss: 0.557650\n",
      "Epoch: 19\n",
      "\tLoss: 0.194395 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.241787 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.205848 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.231758 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.190011 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.179281 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.153798 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 82.7%\n",
      "\tAverage loss: 0.543417\n",
      "Epoch: 20\n",
      "\tLoss: 0.107344 [ 6400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.220684 [12800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.160263 [19200 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.260103 [25600 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.298825 [32000 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.149960 [38400 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tLoss: 0.336429 [44800 / 45000]\n",
      "\t\tLearning rate: 0.010000\n",
      "\tAccuracy: 83.1%\n",
      "\tAverage loss: 0.526716\n"
     ]
    }
   ],
   "source": [
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch: {t + 1}\")\n",
    "    train(train_loader, model, metric, optimizer)\n",
    "    test(valid_loader, model, metric)\n",
    "\n",
    "torch.save(model.state_dict(), \"densenet121.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57b2a419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T02:46:43.110623Z",
     "iopub.status.busy": "2023-09-14T02:46:43.109690Z",
     "iopub.status.idle": "2023-09-14T02:46:48.846059Z",
     "shell.execute_reply": "2023-09-14T02:46:48.844969Z",
     "shell.execute_reply.started": "2023-09-14T02:46:43.110578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 84.2%\n",
      "\tAverage loss: 0.492691\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, model, metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
