{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We implement Krizhevsky's AlexNet, originally developed for the ImageNet contest, as a modification of the architecture in the following [image](https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg). We apply it to the CIFAR10 dataset.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-09-11T03:45:37.494660Z","iopub.execute_input":"2023-09-11T03:45:37.494920Z","iopub.status.idle":"2023-09-11T03:45:44.201471Z","shell.execute_reply.started":"2023-09-11T03:45:37.494896Z","shell.execute_reply":"2023-09-11T03:45:44.200508Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = (\n    \"cuda\" if torch.cuda.is_available() else\n    \"mps\" if torch.backends.mps.is_available() else\n    \"cpu\"\n)\n\nprint(f\"Using {device}\")\n\nif device == \"cuda\":\n    print(torch.cuda.get_device_name(torch.cuda.current_device()))","metadata":{"execution":{"iopub.status.busy":"2023-09-11T03:45:44.203437Z","iopub.execute_input":"2023-09-11T03:45:44.203893Z","iopub.status.idle":"2023-09-11T03:45:44.292980Z","shell.execute_reply.started":"2023-09-11T03:45:44.203860Z","shell.execute_reply":"2023-09-11T03:45:44.291954Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using cuda\nTesla T4\n","output_type":"stream"}]},{"cell_type":"code","source":"class AlexNet(nn.Module):\n    # in_channels: number of input channels\n    def __init__(self, in_channels: int) -> None:\n        super().__init__()\n\n        self.relu = nn.ReLU()\n        # Assume input is 32 x 32 x 3\n        # The original implementation had kernel_size=11 and stride=4\n        # 32 x 32 x 96\n        self.conv0 = nn.Conv2d(in_channels, 96, kernel_size=5, padding=2)\n        # Original implementation maxpools had kernel_size=3\n        # 16 x 16 x 96\n        self.maxpool0 = nn.MaxPool2d(kernel_size=2, stride=2)\n        # 16 x 16 x 256\n        self.conv1 = nn.Conv2d(96, 256, kernel_size=5, padding=2)\n        # 8 x 8 x 96\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        # 8 x 8 x 384\n        self.conv2 = nn.Conv2d(256, 384, kernel_size=5, padding=2)\n        # 8 x 8 x 384\n        self.conv3 = nn.Conv2d(384, 384, kernel_size=5, padding=2)\n        # 8 x 8 x 256\n        self.conv4 = nn.Conv2d(384, 256, kernel_size=5, padding=2)\n        # 4 x 4 x 256\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.dropout = nn.Dropout(p=0.5)\n        # We make the layers have output 256 versus the original 4096\n        self.layer0 = nn.Linear(4096, 256)\n        self.layer1 = nn.Linear(256, 256)\n        self.layer2 = nn.Linear(256, 10)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.conv0(x)\n        x = self.relu(x)\n        x = self.maxpool0(x)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool1(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.relu(x)\n        x = self.conv4(x)\n        x = self.relu(x)\n        x = self.maxpool2(x)\n        x = x.reshape(x.shape[0], (-1))\n        x = self.layer0(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.layer1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.layer2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-11T03:48:51.309541Z","iopub.execute_input":"2023-09-11T03:48:51.309911Z","iopub.status.idle":"2023-09-11T03:48:51.325355Z","shell.execute_reply.started":"2023-09-11T03:48:51.309881Z","shell.execute_reply":"2023-09-11T03:48:51.324039Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"transform_CIFAR10 = transforms.Compose([\n    transforms.ToTensor(),\n    # Mean and standard deviation for CIFAR10 dataset\n    # Sourced from gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151\n    transforms.Normalize(\n        mean=[0.4914, 0.4822, 0.4465],\n        std=[0.2470, 0.2435, 0.2616]\n    )\n])\n\ntrain_data = datasets.CIFAR10(\n    root=\"data\",\n    train=True,\n    transform=transform_CIFAR10,\n    download=True\n)\n\ntest_data = datasets.CIFAR10(\n    root=\"data\",\n    train=False,\n    transform=transform_CIFAR10,\n    download=True\n)\n\nBATCH_SIZE = 64\nVALID_SIZE = 0.9\n\ntrain_indices = list(range(len(train_data)))\nnp.random.shuffle(train_indices)\nvalid_split = int(len(train_data) * VALID_SIZE)\nvalid_indices = train_indices[valid_split:]\ntrain_indices = train_indices[:valid_split]\nvalid_data = Subset(train_data, valid_indices)\ntrain_data = Subset(train_data, train_indices)\n\nprint(f\"Training data: {len(train_data)}\")\nprint(f\"Validation data: {len(valid_data)}\")\nprint(f\"Test data: {len(test_data)}\")\n\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T03:48:56.357921Z","iopub.execute_input":"2023-09-11T03:48:56.358277Z","iopub.status.idle":"2023-09-11T03:48:57.887672Z","shell.execute_reply.started":"2023-09-11T03:48:56.358248Z","shell.execute_reply":"2023-09-11T03:48:57.886686Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nTraining data: 45000\nValidation data: 5000\nTest data: 10000\n","output_type":"stream"}]},{"cell_type":"code","source":"EPOCHS = 20\nLR = 0.01\nWEIGHT_DECAY = 0.001\nMOMENTUM = 0.9\n\nmodel = AlexNet(3)\nmodel = nn.DataParallel(model)\nmodel.to(device)\n\nmetric = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(\n    model.parameters(),\n    lr=LR,\n    weight_decay=WEIGHT_DECAY,\n    momentum=MOMENTUM\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T03:49:09.409022Z","iopub.execute_input":"2023-09-11T03:49:09.409400Z","iopub.status.idle":"2023-09-11T03:49:09.509304Z","shell.execute_reply.started":"2023-09-11T03:49:09.409349Z","shell.execute_reply":"2023-09-11T03:49:09.508396Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def train(\n    loader: DataLoader,\n    model: nn.Module,\n    metric: nn.Module,\n    optimizer: torch.optim.Optimizer\n) -> None:\n    total = len(loader.dataset)\n    model.train()\n\n    for batch, (x, y) in enumerate(loader):\n        x = x.to(device)\n        y = y.to(device)\n\n        pred = model(x)\n        loss = metric(pred, y)\n\n        if batch % 100 == 99:\n            progress = (batch + 1) * len(x)\n            print(f\"\\tLoss: {loss.item():>7f} [{progress:>5d} / {total:>5d}]\")\n            print(f\"\\t\\tLearning rate: {optimizer.param_groups[0]['lr']:>8f}\")\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T03:49:11.171753Z","iopub.execute_input":"2023-09-11T03:49:11.172131Z","iopub.status.idle":"2023-09-11T03:49:11.180355Z","shell.execute_reply.started":"2023-09-11T03:49:11.172091Z","shell.execute_reply":"2023-09-11T03:49:11.179396Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def test(\n    loader: DataLoader,\n    model: nn.Module,\n    metric: nn.Module,\n) -> None:\n    total = len(loader.dataset)\n    batch_total = len(loader)\n    total_loss = 0\n    total_correct = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device)\n\n            pred = model(x)\n            total_loss += metric(pred, y).item()\n            pred_correct = pred.argmax(1) == y\n            total_correct += pred_correct.type(torch.float).sum().item()\n\n        total_loss /= batch_total\n        total_correct /= total\n        print(f\"\\tAccuracy: {(100 * total_correct):>0.1f}%\")\n        print(f\"\\tAverage loss: {total_loss:>8f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-11T03:49:11.306740Z","iopub.execute_input":"2023-09-11T03:49:11.307607Z","iopub.status.idle":"2023-09-11T03:49:11.315233Z","shell.execute_reply.started":"2023-09-11T03:49:11.307569Z","shell.execute_reply":"2023-09-11T03:49:11.314181Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for t in range(EPOCHS):\n    print(f\"Epoch: {t + 1}\")\n    train(train_loader, model, metric, optimizer)\n    test(valid_loader, model, metric)\n\ntorch.save(model.state_dict(), \"alexnet.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-09-11T03:49:13.210733Z","iopub.execute_input":"2023-09-11T03:49:13.211092Z","iopub.status.idle":"2023-09-11T03:57:42.734936Z","shell.execute_reply.started":"2023-09-11T03:49:13.211056Z","shell.execute_reply":"2023-09-11T03:57:42.733923Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch: 1\n\tLoss: 2.302569 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 2.111802 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.858279 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 2.042349 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.786326 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.893871 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.700656 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 37.1%\n\tAverage loss: 1.617891\nEpoch: 2\n\tLoss: 1.425370 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.585904 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.771493 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.666008 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.376256 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.598711 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.350612 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 44.9%\n\tAverage loss: 1.396186\nEpoch: 3\n\tLoss: 1.244387 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.372116 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.096970 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.223779 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.972858 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.114952 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.188940 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 52.8%\n\tAverage loss: 1.345214\nEpoch: 4\n\tLoss: 0.993495 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.810140 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.044855 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.904892 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.988333 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 1.167469 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.637290 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 67.4%\n\tAverage loss: 0.930252\nEpoch: 5\n\tLoss: 0.772999 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.821021 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.974401 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.943014 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.689907 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.806605 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.846889 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 70.9%\n\tAverage loss: 0.855086\nEpoch: 6\n\tLoss: 0.672115 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.880573 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.678259 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.777048 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.647365 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.868032 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.530092 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 72.2%\n\tAverage loss: 0.869363\nEpoch: 7\n\tLoss: 0.849322 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.626015 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.525758 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.696806 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.732337 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.527270 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.417898 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 75.3%\n\tAverage loss: 0.719887\nEpoch: 8\n\tLoss: 0.577427 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.678896 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.520410 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.883233 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.599263 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.678762 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.425273 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 76.5%\n\tAverage loss: 0.694763\nEpoch: 9\n\tLoss: 0.402594 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.624769 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.498872 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.580605 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.863766 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.282177 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.602612 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 77.4%\n\tAverage loss: 0.714543\nEpoch: 10\n\tLoss: 0.394503 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.588480 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.351474 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.355747 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.709707 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.361315 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.185242 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 77.7%\n\tAverage loss: 0.712834\nEpoch: 11\n\tLoss: 0.351954 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.299723 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.364873 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.281276 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.279329 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.313140 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.363342 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 77.5%\n\tAverage loss: 0.747514\nEpoch: 12\n\tLoss: 0.204018 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.250657 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.330091 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.557225 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.252730 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.546962 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.436066 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 79.2%\n\tAverage loss: 0.679761\nEpoch: 13\n\tLoss: 0.165596 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.176289 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.264598 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.204724 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.216715 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.387513 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.203356 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 78.3%\n\tAverage loss: 0.703407\nEpoch: 14\n\tLoss: 0.155228 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.291643 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.580922 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.205929 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.254749 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.205495 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.139940 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 79.5%\n\tAverage loss: 0.729738\nEpoch: 15\n\tLoss: 0.295878 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.084229 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.069549 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.173520 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.110152 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.226713 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.361298 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 80.5%\n\tAverage loss: 0.670658\nEpoch: 16\n\tLoss: 0.145976 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.143357 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.178919 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.145323 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.195937 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.341359 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.213387 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 80.0%\n\tAverage loss: 0.741613\nEpoch: 17\n\tLoss: 0.211132 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.161823 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.210013 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.107633 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.083428 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.176640 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.098858 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 80.3%\n\tAverage loss: 0.735196\nEpoch: 18\n\tLoss: 0.137960 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.120316 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.177410 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.106302 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.123616 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.145643 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.208337 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 79.9%\n\tAverage loss: 0.724216\nEpoch: 19\n\tLoss: 0.127321 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.046791 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.201175 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.111898 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.204129 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.239144 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.159844 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 79.3%\n\tAverage loss: 0.796983\nEpoch: 20\n\tLoss: 0.033297 [ 6400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.060338 [12800 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.089952 [19200 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.054586 [25600 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.105968 [32000 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.161633 [38400 / 45000]\n\t\tLearning rate: 0.010000\n\tLoss: 0.032395 [44800 / 45000]\n\t\tLearning rate: 0.010000\n\tAccuracy: 79.6%\n\tAverage loss: 0.774688\n","output_type":"stream"}]},{"cell_type":"code","source":"test(test_loader, model, metric)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T03:57:49.562149Z","iopub.execute_input":"2023-09-11T03:57:49.562527Z","iopub.status.idle":"2023-09-11T03:57:54.540065Z","shell.execute_reply.started":"2023-09-11T03:57:49.562495Z","shell.execute_reply":"2023-09-11T03:57:54.539051Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\tAccuracy: 79.1%\n\tAverage loss: 0.800118\n","output_type":"stream"}]}]}